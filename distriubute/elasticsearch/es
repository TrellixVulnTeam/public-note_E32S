零、开始
    1.安装
    2.配置
        ./config/elasticsearch.yml为主配置文件，包括以下配置
            network.host:<ip>       # 监听的ip地址
    ./bin/elasticsearch -d 以守护进程的形式启动

一、数据输入和输出
    为了存储和发送一个对象，用JSON标准来描述对象。当一个对象被序列化为JSON后，称其为JSON文档。
    在es中，每一个字段都是默认被索引的，并且每个字段都被设置了反向索引。
    1.文档
        我们使用的术语 对象 和 文档 是可以互相替换的。
        不过，有一个区别： 一个对象仅仅是类似于 hash 、 hashmap 、字典或者关联数组的 JSON 对象，对象中也可以嵌套其他的对象。 
        但有一个区别，es的文档仅仅指顶层对象，这个对象已被序列化为JSON，并存储与es中，指定了唯一ID。
    2.文档元数据
        元数据通常是指数据的信息，es的文档元数据包括：
        1)._index, 文档存放的index
            一个index通常是相同特性的数据所分组的一个集合。虽然一个index中也可以有完全不相关的数据，但是这认为是反模式的。
            实际上，数据通过分片实现存储和索引，一个Index只是一个逻辑空间，由一个或多个分片共同组成。
        2)._type, 文档表示的对象类别
            对索引中的文档再进行一级分组。相当于index是一级分类，type是二级分类。一个type下的文档存储基本类似的数据结构。
            type也是属于逻辑分区。
        3)._id, 文档唯一标识
            ID是一个“字符串”，可以是任意字符串。创建一个新的文档，要么提供自己的 _id ，要么让 Elasticsearch 自动生成。
            对id的要求是URL-SAFE，BASE64编码的，长度为20个字符。
    3.文档操作
        一个文档的index、type、id唯一标识一个文档。
        1).创建文档
            自定义id请求:
                PUT /<index>/<type>/<id>
                {
                    "field": "value",
                    ...
                }
            es自动生成id请求:
                POST /<index>/<type>
                {
                    "field": "value",
                    ...
                }
            响应:
                {
                    "_index":    "<index>",
                    "_type":     "<type>",
                    "_id":       "<id>",
                    "_version":  <version>,
                    "created":   <created>
                }
            需要注意，由于es的修改文档的特性和新增文档是完全一模一样的，因此很难区别一个操作是添加新文档，还是修改老文档，操作中存在风险。
            为了确保当前操作是生成新文档，可以进行如下动作：
                a. 最简单的就是采用es自动生成id的形式，这样的文档由于id没有冲突，不会覆盖老文档。
                b. 若需要用自定义id，并且希望es在/index/type/id唯一时才创建，需要显示指定当前的操作:
                    PUT /<type>/<index>/<id>?op_type=create
                    或 PUT /<type>/<index>/<id>?_create
                    若已经存在该文档，则会返回409状态码
        2).读取文档
            可以加上pretty参数让返回的结果更易读(也就是格式化显示的json数据)
                GET /<index>/<type>/<id>?pretty
                {
                    "_index" :   "<index>",
                    "_type" :    "<type>",
                    "_id" :      "<id>",
                    "_version" : <version>,
                    "found" :    <found>,       // 标明是否有被找到的文档
                    "_source" :  {
                        "field": "value",
                        ...
                    }
                }
            source参数可以指定返回中的哪些信息，默认是返回文档中的所有信息。
                GET /<index>/<type>/<id>?_source=<key1>,<key2>,...
            如果只需要"_source"，不需要其他文档元数据，则直接
                GET /<index>/<type>/<id>/_source
        3).检查文档是否存在
            通过发送HEAD请求，检查响应状态码即可判断文档是否存在。
                curl -i -XHEAD http://localhost:9200/<index>/<type>/<id>
            若存在，返回200
            若不存在，返回404
        4).更新文档
            es中的文档数据，是不可以被改变的！
            需要更新文档，es的操作是删除原文档，再生成一个新的文档，新的文档版本号在老文档版本号的基础上递增。
            只需要指定新创建文档的index，type，id为老文档即可实现此类覆盖性更新。
            在es内部，老文档会被标记为删除，但是不会立即删除，而是由es后台进程在合适的时间进行删除。
            需要注意，文档中有一个"created"的元数据，该数据标记该文档是否为初始文档。
            # 部分更新
                很明显，在更新文档的时候，需要先获取文档的数据，在进程中对文档的数据进行更新，然后再调用更新接口。
                这个操作中包含了先获取文档数据的步骤，这会多一次网络请求。
                ”部分更新“可以指定对文档的字段进行更新，不用事先检索文档，检索操作在分片内部进行，然后修改，覆盖。
                部分更新的本质，就是将文档检索出来后，和请求中的参数进行合并。
                    POST /<index>/<type>/<id>/_update
                    {
                    "doc" : {
                        "tags" : [ "testing" ],
                        "views": 0
                    }
                    }
        5).删除文档
            DELETE /<index>/<type>/<id>
    4.乐观并发控制
        这类数据源的数据文件存在竞争情况，尤其是“读--计算--写入”这个流程下更是常见的竞争场景。
        在"读-计算-写入"的过程中，如果数据有被更新，那么计算的则是以前的脏数据，也就更不应该以这个脏数据计算出来的结果进行写入。
            悲观锁，会对整个“读-计算-写入”过程上锁，只有当写入完成后，才会释放锁，其他的业务才能进行接入，这类方案锁粒度大，通常并发效率不高。
            乐观锁，会在写入时判断数据是否已经失效，若已经失效，则放弃写入。一般需要重新读出，重新计算。此类对计算能力有些许浪费。
        es通过版本号的机制，实现了乐观锁。每当更新文档，都会更新版本号。采用乐观锁，需要在url中显示指定基于版本号的更新条件。
        当es中的文档版本为<version>时，才进行更新:
            PUT /<index>/<type>/<id>?version=<version>
        # 外部版本号：
            内部版本号是在文档版本和指定版本相同时进行更新，新文档的版本号为内部版本号的递增。
            外部版本号是在文档版本比指定的版本号小时更新，新文档的版本号为指定的外部版本号。
            使用外部版本号需要显示指定：
                PUT /<index>/<type>/<id>?version=5&version_type=external
            外部版本号的使用，是为了方便使用timestamp等数据作为版本号而使用的。
            例如MySQL的数据更新时，可以记录更新的timestamp，用这个timestamp作为版本号更新es中的数据。
    5.批量操作
        将多个请求合并成一个请求，可以进行效率更高的操作，这样节省了网络延时。
        1.读的批量操作
            主要是通过mget api实现这样的操作
                GET /_mget
                {
                    "docs" : [
                        {
                            "_index" : "<index1>",
                            "_type" :  "<type1>",
                            "_id" :    <id1>
                        },
                        {
                            "_index" : "<index2>",
                            "_type" :  "<type2>",
                            "_id" :    <id2>
                            "_source": "views"          # 指定返回的参数名
                        }
                    ]
                }
            若index相同 或 index和type相同，可以简化jsonbody
                GET /<index>/<type>/_mget
                {
                    "docs" : [
                        { "_id" : <id1> },                      # 检索/index/type/id1
                        { "_type" : "<type2>", "_id" :<id2> }   # 检索/index/type2/id2
                    ]
                }
            若index和type都相同，可以只传ids数组
                GET /website/blog/_mget
                {
                    "ids" : [ "2", "1" ]
                }
        2.增删改批量
            使用bulk api。
            bulk请求体格式：
                { <action>: <metadata-json> }\n
                <source-json>\n
                { <action>: <metadata-json> }\n
                <source-json>\n
                ...
            每行一定要以换行符(\n)结尾， 包括最后一行 。这些换行符被用作一个标记，可以有效分隔行。
            <action>指定操作的类型：
                create，如果文档不存在，那么就创建它。
                index，创建一个新文档或者替换一个现有的文档。
                update，部分更新一个文档。
                delete，删除一个文档。
            <meta-json>中指定文档的位置，采用json格式：
                { "_index": "<index>", "_type": "<type>", "_id": "<id>" }
            <source-json>是增，改必须指定的。删除操作不用指定该数据。

            每个子请求都是独立执行，因此某个子请求的失败不会对其他子请求的成功与否造成影响。 
            如果其中任何子请求失败，最顶层的 error 标志被设置为 true ，并且在相应的请求报告出错误明细：
            但是不影响其他请求的结果。
二、分布式文档存储
    这里将会讨论文档如何从集群中存储和获取
    1.文档路由到分片
        集群中有多个主分片，文档是直接存储在分片中的，无论集群中的哪个节点，接收到一个文档请求时，首先都会计算这个文档属于哪个分片，进而计算出在哪个节点。
        文档到分片的路由：
            shard = hash(routing) % number_of_primary_shards
        其中，routing默认是文档的id。
    2.主分片和副分片的交互
        相同分片的副本不会放在同一节点。
        我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。
        需要注意，对于修改操作(增删改)，节点会把请求转发到主分片的节点上进行处理，主分片处理完成后，会把请求再转发给副分片所在的node进行处理，即同步更新。
        接受client请求的节点被称为协调节点，任何节点都可以是协调节点。
    3.新建、索引和删除文档
        1、客户端向 Node 1 发送新建、索引或者删除请求。
        2、节点使用文档的_id 确定文档属于分片0。主分片0所在的节点。
        3、在主分片上面执行请求。如果成功了，它将请求并行转发到副本分片所在的节点。一旦所有的副本分片都报告成功, 主分片将向协调节点报告成功，协调节点向客户端报告成功。
        在默认设置下，即使仅仅是在试图执行一个写操作之前，主分片都会要求必须要有规定数量的分片副本处于活跃可用状态，才会去执行写操作(其中分片副本可以是主分片或者副本分片)。
        这是为了避免在发生网络分区故障（network partition）的时候进行写操作，进而导致数据不一致。
        规定数量即：
            int( (primary + number_of_replicas) / 2 ) + 1
    4.获取一个文档
        1、客户端向 Node1 发送获取请求。
        2、节点使用文档的 _id 来确定文档属于分片0。分片0的副本分片存在于所有的多台节点上。在这种情况下，它将通过RR负载均衡方案发送给其中一台节点。
        3、接收请求处理的节点 将文档返回给 Node1 ，然后 Node1 将文档返回给客户端。
        在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。
        在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。
    5.局部更新一个文档
        1、客户端向 Node1 发送更新请求。
        2、它将请求转发到主分片所在的Node。
        3、从主分片检索文档，修改 _source 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict 次后放弃。
        4、如果成功地更新主分片的文档，它将新版本的文档并行转发到副本分片的节点上，重新建立索引。 一旦所有副本分片都返回成功， 主分片节点向协调节点也返回成功，协调节点向客户端返回成功。
    6.多文档模式
         协调将整个多文档请求分解成每个分片的多文档请求，并且将这些请求并行转发到每个参与节点。
         协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端
         1) mget
            将会转发给主分片或者副本分片的节点上
         2) bulk
            并行转发到每个包含主分片的节点主机

三、搜索
    0.基本概念
        映射（Mapping）
            描述数据在每个字段内如何存储
        分析（Analysis）
            全文是如何处理使之可以被搜索的
        领域特定查询语言（Query DSL）
            Elasticsearch 中强大灵活的查询语言
    1.空搜索
        没有指定任何查询条件的空搜索，返回集群中所有的index下的文档。
            GET /_search
            响应body：
                {
                    "hits" : {
                        "total" : <total>           # 命中的个数
                        "hits" : [                  # hits 数组中每个结果包含文档的 _index 、 _type 、 _id ，加上 _source 字段
                            ... 10 RESULTS REMOVED ...
                        ],                          # 每个结果还有一个 _score ，它衡量了文档与查询的匹配程度。
                                                    # 默认情况下，首先返回最相关的文档结果，就是说，返回的文档是按照 _score 降序排列的。
                        "max_score" : 1             # max_score 值是与查询所匹配文档的 _score 的最大值。
                    },
                    "took" : <cost-time>,           # 本次请求花费的时间(ms)
                    "_shards" : {                   # 查询中参与分片的总数，以及这些分片成功了多少个失败了多少个。
                        "failed" : 0,
                        "successful" : 10,
                        "total" : 10
                    },
                    "timed_out" : false             # 查询是否超时, 可以在请求中指定超时时间:GET /_search?timeout=10ms
                }
    2.多索引 多类型
        /_search
            在所有的索引中搜索所有的类型
        /<index>/_search
            在 <index> 索引中搜索所有的类型
        /<index1>,<index2>/_search
            在 <index1>,<index2> 索引中搜索所有的文档
        /g*,u*/_search
            在任何以 g 或者 u 开头的索引中搜索所有的类型
        /gb/user/_search
            在 gb 索引中搜索 user 类型
        /<index1>,<index2>/<type1>,<type2>/_search
            在 <index1>,<index2> 索引中搜索 <type1>,<type2> 类型
        /_all/<type1>,<type2>/_search
            在所有的索引中搜索 <type1>,<type2> 类型
        当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的--只是会涉及到更多的分片。
    3.分页
        Elasticsearch 接受 from 和 size 参数：
            size
                显示应该返回的结果数量，默认是 10
            from
                显示应该跳过的初始结果数量，默认是 0
            GET /_search?size=5
            GET /_search?size=5&from=5
            GET /_search?size=5&from=10
        一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。
    4.轻量搜索
        es可以将简单的搜索条件放在url中，这样的搜索被称为轻量搜索:
            GET /<index>/<type>/_search?q=<key>:<val>
        在指定的index，type中，搜索key中包含<val>的文档。
        # _all字段
            直接用q=<val>，忽略<key>，代表从文档的_all字段中查询<val>。
                GET /<index>/<type>/_search?q=<val>
            _all字段，代表将文档中的所有数据，采取字符串进行拼接后的一个字段。
            因此，从_all字段查询，代表从指定位置中查询出所有包含<val>的文档。
            需要注意的是，这是一个字符串，因此会是全文检索(非精确值匹配)
四、映射和分析
五、请求体查询
    简易 查询 —query-string search— 对于用命令行进行点对点（ad-hoc）查询是非常有用的。 
    然而，为了充分利用查询的强大功能，你应该使用 请求体 search API， 
    之所以称之为请求体查询(Full-Body Search)，因为大部分参数是通过 Http 请求体而非查询字符串来传递的。
    一个带请求体的查询允许我们使用 查询领域特定语言（query domain-specific language） 或者 Query DSL 来写查询语句。
    1.空查询
        查询可以用带body的GET请求或直接用POST请求。
        GET请求更能表示“查询”，但是带body的GET请求并非标准，因此只有部分http服务器支持，所以es也添加了对post查询的支持。
            GET/POST /_search
            {
                "from": 30,     # 忽略时，默认为0
                "size": 10      # 忽略时，默认为10
            }
    2.查询表达式
        查询表达式写在body中：
            {
                "query": <YOUR_QUERY_HERE>
            }
        其中query字段中存放查询语句。
        <YOUR_QUERY_HERE> 查询语句结构：
            {
                <query>: {
                    <key1>: <val1>,
                    <key2>: <val2>,
                    ...
                }
            }
        完整的body结构应该时：
            {
                "query": {
                    <query>: {
                        <key1>: <val1>,
                        <key2>: <val2>,
                        ...
                    }
                }
            }
    3.常用的查询
        es带了很多查询方式，但是常用的查询就只有几种。
        1).match_all
            在没有指定查询方式时，它是默认的查询。
        2).match
            在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串。
            如果是在精确字段上使用 match 查询，将会精确匹配给定的值。
        3).multi_match
            在多个字段上执行相同的match操作.
            在<key1>和<key2>中查询<val>
                {
                    "multi_match": {
                        "query":    "<val>",
                        "fields":   [ "<key1>", "<key>" ]
                    }
                }
        4).range
            range 查询找出那些落在指定区间内的数字或者时间:
            查询<key>在<op1>和<op2>指定的区间中。
                {
                    "range": {
                        "<key>": {
                            "<op1>": <val1>,
                            "<op2>": <val2>
                        }
                    }
                }
        5).term
            term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串：
        6).terms
            terms 查询和 term 查询一样，但它允许你指定多值进行匹配。这个字段包含了指定值中的任何一个值，那么这个文档满足条件。
        7).exist
            exists 被用于查找那些指定字段中“有值”的文档
        8).missing
            missing 被用于查找那些指定字段中“无值”的文档
    4.组合多查询
        bool字段可以将多个查询进行组合，接受以下参数
            must
                文档 必须 匹配这些条件才能被包含进来。
            must_not
                文档 必须不 匹配这些条件才能被包含进来。
            should
                如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分。(我感觉增加的并不明显，还是需要用boost来提升)
            filter
                必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。
        每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。
六、排序与相关性
    3.相关性
        每个文档都有相关性评分，用一个正浮点数字段_score 来表示。_score 的评分越高，相关性越高。
        评分的计算方式取决于查询类型，fuzzy查询会计算与关键词的拼写相似程度，terms 查询会计算找到的内容与关键词组成部分匹配的百分比
        1).检索词频率
            检索词在字段中出现的频率，出现的越多，相关性越高。
        2).反向文档频率
            检索词在整个索引中的出现频率，出现的越多，相关性越低。
        3).字段长度准则
            字段的文字总长度越长，相关性越低。
        可以通过explain的请求，获取一次查询中较为相信的相关度的信息。
七、分布式检索
八、索引管理
    索引管理可以优化索引和搜索过程
九、分片内部原理
十、结构化搜索
十一、全文搜索
十二、多字段搜索
十三、近似匹配
十四、部分匹配
十无、控制相关度
    全文相关的公式或相似算法，会将多个因素合并起来，为每个文档生成一个相关度评分_score。
    1.基础理论
        lucene和es都是通过bool模型匹配文档，并用一个“实名评分函数”计算文档的相关度。相关度的计算，基于
        词频/逆向文档频率 和 向量空间模型，同时也加入了一些现代的新特性，如协调因子，字段长度归一化，以及词或查询语句权重提升。
        1).布尔模型(bool)
            将查询条件通过AND，OR，NOT进行逻辑组合，快速简单的将不匹配的文档排除掉。
        2).相关性计算
            * 词频(TF)
                TF，表示检索的词在文档中出现的频次, TF越高，相关性越高:
                    tf(t in d) = sqrt(frequency)
                tf(t in d) 表示词t在文档d中的出现次数
                frequncy表示词t在字段中的出现次数
            * 逆向文档频率(IDF)
                词在“文档”中出现的频率越高，说明这个词很常见，因此相关性越低。
                    idf(t) = 1 + log ( numDocs / (docFreq + 1))
            * 字段长度归一值
                字段的长度越短，词的相关性越高
                    norm(d) = 1 / √numTerms 
            * 向量空间模型
                向量空间主要用于多词查询，每个词都会在向量中占一纬。有N个词，向量空间即N维。
                向量有两种：
                    - 查询向量
                        每个词根据IDF计算一个N维向量，作为查询向量。
                    - 文档向量
                        每个词都会计算在文档中的相关性，并将这个相关性拼成一个N维向量。
                计算查询向量和每个文档向量的匹配程度(角度)，计算出相关性得分。
    2.Lucene评分函数
        多词查询的相关性计算流程：
            score(q,d)  =               # 计算关键词q在文档d中的相关性
                queryNorm(q)            # 查询归一化因子
                coord(q,d)              # 查询协调因此
                ∑ (                     # 查询q中每个词t
                    tf(t in d)          # t在文档d中的词频
                    idf(t)²             # t的逆向文档频率
                    t.getBoost()        # t是否进行boost(查询时权重提升)
                    norm(t,d)           # 字段长度归一值
                ) (t in q)              # 将相关性进行求和
        * 查询归一因子
        * 查询协调
        * 权重提升
            可以指定特定字段提升权重。权重提升有两个时机：建立索引时 以及 查询时。
            建议最好查询时提升，因为这样更为灵活。
    3.查询时权重提升
        {
            ...
            "match": {
                # 指定对title字段进行提升
                "title": {
                    "query": key_words,
                    "boost": <boost>
                }
          }
            ...
        }

    4.查询结构修改相关度