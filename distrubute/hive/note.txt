一、简介
	目的是让精通SQL的人呢过个轻易查看和使用hdfs的数据。
	hive的目标是一个数据仓库，而非数据库。数据仓库是一个对实时要求不高，对更新要求不高的数据存储仓库。
	1.与传统数据库的比较
		1).读模式与写模式
		2).更新
		3).事务
		4).索引
	2.HiveService
		Hive Client ---> Hive Service ---> Hadoop
		Hadoop，用来对数据进行保存，并执行hql语句的分布式平台。
		Hive Service，用于接收用户的hql语句，并将hql语句转为mapreduce并提交给Hadoop。并且Service可以采用HA机制来部署。元数据的存储是交给Hive Service负责的。
		Hive Client，一般是指的JDBC/ODBC用户。
	3.示例
		1).建表
			create table <table-name> (<field1> <type1>, <field2> <type2>, ...)
			row format delimited
			fields terminated BY '\t';
			创建表，row format指行分割符为换行符，fields terminated by指定行内的列分隔符，'\t'为制表符，也就是以制表符作为列分割。
		2).加载数据
			load data local inpath '<path>'
			overwrite into table <table-name>
			Hive把指定的本地文件夹放到hive仓库目录中，这个只是单纯的复制而已，并不会解析文件。文件不会被修改的。
二、HiveQL
	1.数据类型
		1).简单类型
			TINYINT  	DOUBLE
			SMALLINT	BOOLEAN
			INT			STRING
			BIGINT		BINARY
			FLOAT		TIMESTAMP
		2).复杂类型
			ARRAY	一组有序字段，字段类型必须相同
			MAP		一组无序的kv对集合。集合中的kv类型均相同。
			STRUCT	一个对象，或者理解为命名的字段。
			如 create table <table-name>{
				c1 ARRAY<INT>,
				c2 MAP<STRING, INT>,
				c3 STRUCT<a:STRING, b:INT, c:DOUBLE>
			}
			select c1[2], c2['key'], c3.c from table
		3).强制转换
			任何数据类型都可以隐式的转换为一个范围更广的类型，如FLOAT和DOUBLE的进行运算，会自动将FLOAT转换为DOUBLE再进行运算。
			也可以通过cast进行强制转换：cast(val as <new-type>) 将val转换为<new-type>下的值，若不能转换就会返回null。
	2.表
		表由存储的<数据>和表结构的<元数据>组成。数据一般放在HDFS中，元数据一般放在关系型数据库中(如Derby和MySQL)。
		创建表时，默认情况下Hive负责管理数据，意味着Hive会把数据复制到Hive目录下。还可以选择外部表，这会让Hive把数据复制到指定目录下。
		create table <tbname> (...);								//创建表
		create external table <tbname> (...) location '<new-path>'	//创建外部表，建表时不会检查<new-path>是否真的存在。
		load data inputh '<path>' into table <tbname>;				//加载数据到Hive目录
		drop table <tbname>;
		删除表时，会把表的元数据和数据一起删除。
		删除外部表时，只会删除元数据。
	3.分区和桶
		分区和桶，都是加快检索速度的机制。分区是将不同的值进行划分，分桶是将不同的hash值进行分"区"，因此本质上分桶和分区是一样的。
		1.分区
			创建分区表: 
				create table <tbname> (...)
				partitioned by (<partion-field1> <type1>, <partion-field2> <type2>, ...);
			加载数据:
				load data local inpath '<path>' into table <tbname>
				partition (<partition-field1>='<value1>', <partition-field2>='<value2>', ...)
			一个表可以从多个字段进行分区，分区采用的关键字为partition by。分区会在对应的表目录下面在生成子目录，以保存对应分区的数据。
			tbpath/partition1-value1/partition2-value1/...
			                        /partition2-value2/...
			      /partition1-value2/partition2-value1/...
				                    /partition2-value2/...
			分区中的列也是属于表元素句的，但是列值的获取并不通过数据文件(数据文件中也没有)，而是通过文件目录来获取。对应目录下的数据是在加载数据的时候赋予的。
		2.桶
			创建带桶的表：create table <tbname> (...)
						  clustered by (<cluster-field>) into <int> buckets;
			将表以<cluster-field>进行创建<int>个桶。
			不同的桶以文件的形式存在。
			桶中的列数据是在元数据中存在的(也没办法从桶号码恢复出列数据，因为这相当于是hash恢复出原数据的难度，不可能实现)
	4.存储格式
		SerDe指的是serialize/deserialize，即序列化和反序列化工具。
	5.表的数据导入
		1).load data
			load data inpath '<file-path>' 
			[overwrite] into table <tbname>
			row formated delimited
				fields terminated by <fields-split-symbol>						//
				collection items terminated by <collection-split-symbol>		//array struct 及 mapkv对 中的分隔符。
				map keys interminated by <kv-split-symbol>;						//mapkv的分割符。
			overwrite将会删除表对应目录中已有的所有文件，如果省去这一关键字，则简单地把新的文件加入目录，若有同名文件采进行覆盖。
		2).insert
			insert overwrite table <tbname>
			select <field1> <field2> from <other-tbname>; overwrite指用查询结果覆盖掉<tbname>中的内容.
			
			insert overwrite table <tbname> partition (<partion-field>='value')
			select <field1> <field2> from <other-tbname>; 将查询结果覆盖掉指定分区中的内容。
		3).多表插入
			还可以将一个表插入到多个表中，并且可以指定每个表中的数据表现形式。
			from <tbname>
			insert overwrite table <tbname1>
				select ...
			insert overwrite table <tbname2>
				select ...
		4).CTAS
			即create table ... as select;用于将Hive的查询结果放入新表。
			如create table <new-tbname> as select <field1> <field2> ... from <old-tbname>;
			即将对<old-tbname>的查询结果放入<new-tbname>中。
	6.表的修改
		alter table <old-tbname> rename to <new-tbname>;		修改表名，除了修改元数据外，还会将文件目录进行重命名。
		alter table <tbname> add columns (<new-col> <type>);	添加一个新列，通常原数据中不包含新列，因此此时新列的值为null。可以将一个定义了新列值的表覆盖掉原有的表。
		drop table <tbname>，用于删除表和元数据，若是外部表就只删除元数据。若保留元数据，但是删除数据，则删除数据文件即可。
	7.查询数据
		1).排序和聚集
			order by 通过一个reduce来排序。对于大规模数据而言，仅用一个reduce就会让效率很低下。
			sorted by 为每个reducer产生一个排序文件。
		2).连接
			a).内连接
				通过join on实现，on是连接的条件，hive中只支持等值连接。
				在查询中可以使用多个join on来连接多个表，hive会以最少的mapreduce作业数来执行连接。
				如果多个的连接的连接条件使用的列相同，那么可以减少mapreduce作业数。
				如：select a.*, b.* from a join b on a.id = b.id;
				在查询语句前面加explain可以查询执行计划的详细信息。
			b).外连接
				外连接分了左外连接 和 右外连接。
				left  outer join 左外连接
				right outer join 右外连接
				full  outer join 全外连接
			c).半连接
				使用LEFT SEMI JOIN来使用。
				主要是为了支持in子查询。
				传统方式：select * from things where things.id in (select id from sales);
				hive方式：select * from things left semi join sales on things.id = sals.id;
			d).map连接
				如果连接的两个表中，有一个表特别小，就可以用map连接。map连接个将特表小的表放入每个mapper的内存，并省略掉reduce的步骤。
				因为若要执行右连接或是全连接时，right表中的某一行和其中一个mapper无法匹配并不代表该行和左表无法匹配，在一个mapper中若右/全连接无法匹配将会把左表的该行输出为null，但其实在其他mapper中可能是匹配的。
				换句话本质就是right表无法得知left表的所有情况，一个mapper中无法确定当前右/全连接当前行没法匹配时左表在该行的输出情况，因此无法进行右连接和全连接。
				select /*+ MAPJOIN(<right-tbname>) */ <left-tbname>.*, <right-tbname>.* from <left-tbname> join <right-tbname> on <left-tbname>.id = <right-tbname>.id;
		3).子查询
			hive的子查询只允许子查询语句出现在select的from子句中。
		4).视图
			视图是一种用select语句定义的虚表。
			视图可以将表进行简化和聚集便于后期处理，也可以用来限制用户访问其权限所能访问的。
			视图只是一条查询语句，并且在创建视图时并不会执行该语句。该语句用来作为子查询。
			create view <view-name> as
			select * from <tbname>;
			
			select * from <view-name>;
			hive中的视图是只读的。
	8.用户定义函数
		hive内置函数不能满足需求时，可以使用用户定义函数。
		1).UDF
			用于单行数据，且产生一个数据作为输出。
		2).UDAF
			用于多行数据，且产生一个数据作为输出。例如count
		3).UDTF
			用于单行数据，且产生多行数据(表)作为输出。